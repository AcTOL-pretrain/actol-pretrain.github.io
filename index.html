<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents - Zhizhen Zhang">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Zhizhen Zhang, Lei Zhu, Zhen Fang, Zi Huang, Yadan Luo">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents">
  <meta name="citation_author" content="Zhang, Zhizhen">
  <meta name="citation_author" content="Zhu, Lei">
  <meta name="citation_author" content="Fang, Zhen">
  <meta name="citation_author" content="Huang, Zi">
  <meta name="citation_author" content="Luo, Yadan">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="NeurIPS 2025">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2502.01218.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents - Zhizhen Zhang | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico"> -->
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=X1sJkM8AAAAJ&hl=en" target="_blank">Zhizhen Zhang<sup>1</sup>,</a></span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=en&user=gw4ISc4AAAAJ" target="_blank">Lei Zhu<sup>2</sup>,</a></span>
                  <span class="author-block">
                    <a href="https://fang-zhen.github.io/index.html" target="_blank">Zhen Fang<sup>3</sup>,</a></span>
                    <span class="author-block">
                      <a href="https://staff.itee.uq.edu.au/huang/" target="_blank">Zi Huang<sup>1</sup>,</a></span>
                      <span class="author-block">
                        <a href="https://luoyadan.github.io/" target="_blank">Yadan Luo<sup>1</sup></a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block"><sup>1</sup>University of Queensland, <sup>2</sup>Tongji University, <sup>3</sup>University of Technology Sydney<br>Accepted by NeurIPS 2025</span>

                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2502.01218.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/Daisy-zzz/AcTOL" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.01218" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body"> -->
      <!-- TODO: Replace with your teaser video -->
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata"> -->
        <!-- TODO: Add your video file path here -->
        <!-- <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video> -->
      <!-- TODO: Replace with your video description -->
      <!-- <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (<b>AcTOL</b>) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments on both simulated and real robots show that the pretrained features significantly enhance downstream manipulation tasks with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents.
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/arch.png" alt="First research result visualization" loading="lazy"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          Comparison of existing goal-reaching pre-training strategies and the proposed AcTOL approach.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/sim_env.png" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Policy learning environments, including 3 tasks with a real-world Unitree D1 robot arm and 5 tasks each in two simulation environments.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/comparison.png" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Comparison of language-conditioned behavior cloning results by AcTOL and existing methods.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/reward.png" alt="Fourth research result visualization" loading="lazy"/>
      <h2 class="subtitle has-text-centered">
        Visualization of the normalized zero-shot reward corresponding to different actions.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Video Gallery -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Gallery</h2>

      <div class="columns is-multiline">
        <!-- Pick up cup -->
        <div class="column is-one-quarter has-text-centered">
          <h3 class="title is-6">Pick up cup</h3>
          <img class="video-thumb" src="static/images/pick_cup.jpg" 
                data-video="static/videos/pick_cup_h264.mp4"
                 muted loop preload="metadata"
                 style="width: 100%; border-radius: 8px; height: 200px; object-fit: cover; cursor: pointer;">
            <source src="static/videos/pick_cup.mp4" type="video/mp4">
          </video>
        </div>

        <!-- Open drawer -->
        <div class="column is-one-quarter has-text-centered">
          <h3 class="title is-6">Open drawer 1</h3>
          <img class="video-thumb" src="static/image/open_drawer_1.jpg" 
          data-video="static/videos/open_drawer_1_h264.mp4"
                 muted loop preload="metadata"
                 style="width: 100%; border-radius: 8px; height: 200px; object-fit: cover; cursor: pointer;">
            <source src="static/videos/open_drawer_1.mp4" type="video/mp4">
          </video>
        </div>

        <div class="column is-one-quarter has-text-centered">
          <h3 class="title is-6">Open drawer 2</h3>
          <img class="video-thumb" src="static/image/open_drawer_2.jpg"
          data-video="static/videos/open_drawer_2_h264.mp4"
                 muted loop preload="metadata"
                 style="width: 100%; border-radius: 8px; height: 200px; object-fit: cover; cursor: pointer;">
            <source src="static/videos/open_drawer_2.mp4" type="video/mp4">
          </video>
        </div>

        <div class="column is-one-quarter has-text-centered">
          <h3 class="title is-6">Open drawer 3</h3>
          <img class="video-thumb" src="static/image/open_drawer_3.jpg"
          data-video="static/videos/open_drawer_3_h264.mp4"
                 muted loop preload="metadata"
                 style="width: 100%; border-radius: 8px; height: 200px; object-fit: cover; cursor: pointer;">
            <source src="static/videos/open_drawer_3.mp4" type="video/mp4">
          </video>
        </div>

        <!-- Close drawer -->
        <div class="column is-one-quarter has-text-centered">
          <h3 class="title is-6">Close drawer 1</h3>
          <video class="video-thumb" data-video="static/videos/close_drawer_1_h264.mp4"
                 muted loop preload="metadata"
                 style="width: 100%; border-radius: 8px; height: 200px; object-fit: cover; cursor: pointer;">
            <source src="static/videos/close_drawer_1.mp4" type="video/mp4">
          </video>
        </div>

        <div class="column is-one-quarter has-text-centered">
          <h3 class="title is-6">Close drawer 2</h3>
          <video class="video-thumb" data-video="static/videos/close_drawer_2_h264.mp4"
                 muted loop preload="metadata"
                 style="width: 100%; border-radius: 8px; height: 200px; object-fit: cover; cursor: pointer;">
            <source src="static/videos/close_drawer_2.mp4" type="video/mp4">
          </video>
        </div>

        <div class="column is-one-quarter has-text-centered">
          <h3 class="title is-6">Close drawer 3</h3>
          <video class="video-thumb" data-video="static/videos/close_drawer_3_h264.mp4"
                 muted loop preload="metadata"
                 style="width: 100%; border-radius: 8px; height: 200px; object-fit: cover; cursor: pointer;">
            <source src="static/videos/close_drawer_3.mp4" type="video/mp4">
          </video>
        </div>

        <div class="column is-one-quarter has-text-centered">
          <h3 class="title is-6">Close drawer 4</h3>
          <video class="video-thumb" data-video="static/videos/close_drawer_4_h264.mp4"
                 muted loop preload="metadata"
                 style="width: 100%; border-radius: 8px; height: 200px; object-fit: cover; cursor: pointer;">
            <source src="static/videos/close_drawer_4.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Modal for enlarged video -->
<div class="modal" id="videoModal">
  <div class="modal-background"></div>
  <div class="modal-content has-text-centered">
    <video id="modalVideo" controls autoplay style="max-width: 90%; max-height: 80%; border-radius: 8px;">
      <source src="" type="video/mp4">
    </video>
  </div>
  <button class="modal-close is-large" aria-label="close"></button>
</div>

<script>
  document.addEventListener("DOMContentLoaded", () => {
    const modal = document.getElementById("videoModal");
    const modalVideo = document.getElementById("modalVideo");
    const modalClose = modal.querySelector(".modal-close");
    const modalBg = modal.querySelector(".modal-background");

    // 点击缩略视频 -> 打开modal并播放
    document.querySelectorAll(".video-thumb").forEach(thumb => {
      thumb.addEventListener("click", () => {
        const videoSrc = thumb.getAttribute("data-video");
        modalVideo.querySelector("source").src = videoSrc;
        modalVideo.load();
        modal.classList.add("is-active");
        modalVideo.play();
      });
    });

    // 关闭modal
    [modalClose, modalBg].forEach(el => {
      el.addEventListener("click", () => {
        modal.classList.remove("is-active");
        modalVideo.pause();
      });
    });
  });
</script>






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      TODO: Replace with your poster PDF
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@misc{zhang2025provableorderingcontinuityvisionlanguage,
      title={Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents}, 
      author={Zhizhen Zhang and Lei Zhu and Zhen Fang and Zi Huang and Yadan Luo},
      year={2025},
      eprint={2502.01218},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2502.01218}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
